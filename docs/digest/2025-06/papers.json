{
  "month": "2025-06",
  "papers": [
    {
      "arxiv_id": "2506.01867v1",
      "arxiv_id_base": "2506.01867",
      "authors": [
        "Mattson Ogg",
        "Rahul Hingorani",
        "Diego Luna",
        "Griffin W. Milsap",
        "William G. Coon",
        "Clara A. Scholl"
      ],
      "categories": [
        "q-bio.NC",
        "eess.SP"
      ],
      "links": {
        "abs": "http://arxiv.org/abs/2506.01867v1",
        "pdf": "https://arxiv.org/pdf/2506.01867v1"
      },
      "published_date": "2025-06-02",
      "summary": {
        "arxiv_id_base": "2506.01867",
        "categories": [
          "q-bio.NC",
          "eess.SP"
        ],
        "data_scale": {
          "channels": null,
          "datasets": [],
          "eeg_hours": null,
          "subjects": null
        },
        "detailed_summary": "Unable to produce a reliable multi-sentence summary due to JSON validation failure.",
        "evaluation": {
          "benchmarks": [],
          "headline_results": [],
          "tasks": []
        },
        "key_points": [
          "unknown",
          "unknown",
          "unknown"
        ],
        "limitations": [
          "unknown",
          "summary_json_error"
        ],
        "method": {
          "architecture": null,
          "finetuning": null,
          "objective": null,
          "pretraining": null
        },
        "notes": "manual_resummary_prompt_update_arcee;input_mode=fulltext;prompt_tokens=9323;summary_json_error",
        "one_liner": "Summary unavailable due to JSON validation failure.",
        "open_source": {
          "code_url": null,
          "license": null,
          "weights_url": null
        },
        "paper_type": "other",
        "published_date": "2025-06-02",
        "tags": {
          "backbone": [],
          "objective": [],
          "paper_type": [],
          "tokenization": [],
          "topology": []
        },
        "title": "EEG Foundation Models for BCI Learn Diverse Features of Electrophysiology",
        "unique_contribution": "unknown",
        "used_fulltext": true
      },
      "summary_failed_reason": null,
      "title": "EEG Foundation Models for BCI Learn Diverse Features of Electrophysiology",
      "triage": {
        "confidence": 1.0,
        "decision": "accept",
        "reasons": [
          "explicitly introduces EEG foundation model pre-training method",
          "focuses on self-supervised objectives for robust EEG representations",
          "evaluates model on standard BCI tasks and electrophysiological features"
        ]
      }
    },
    {
      "arxiv_id": "2506.06353v1",
      "arxiv_id_base": "2506.06353",
      "authors": [
        "Naseem Babu",
        "Jimson Mathew",
        "A. P. Vinod"
      ],
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.ET",
        "cs.HC",
        "cs.LG"
      ],
      "links": {
        "abs": "http://arxiv.org/abs/2506.06353v1",
        "pdf": "https://arxiv.org/pdf/2506.06353v1"
      },
      "published_date": "2025-06-02",
      "summary": {
        "arxiv_id_base": "2506.06353",
        "categories": [
          "eess.SP",
          "cs.AI",
          "cs.ET",
          "cs.HC",
          "cs.LG"
        ],
        "data_scale": {
          "channels": null,
          "datasets": [],
          "eeg_hours": null,
          "subjects": null
        },
        "detailed_summary": "This survey systematically reviews recent advancements in applying large language models (LLMs) to electroencephalography (EEG) analysis. It organizes the literature into four domains: (1) LLM-inspired foundation models for EEG representation learning, (2) EEG-to-language decoding, (3) cross-modal generation including image and 3D object synthesis, and (4) clinical applications and dataset management tools. The survey highlights how transformer-based architectures adapted through fine-tuning, few-shot, and zero-shot learning have enabled EEG-based models to perform complex tasks such as natural language generation, semantic interpretation, and diagnostic assistance. By presenting a structured overview of modeling strategies, system designs, and application areas, this work serves as a foundational resource for future work to bridge natural language processing and neural signal analysis through language models.",
        "evaluation": {
          "benchmarks": [],
          "headline_results": [],
          "tasks": []
        },
        "key_points": [
          "Comprehensive survey of LLM applications in EEG analysis across four domains: foundation models, decoding, cross-modal generation, and clinical applications.",
          "Highlights transformer-based architectures adapted through fine-tuning, few-shot, and zero-shot learning for complex EEG tasks.",
          "Organizes recent studies into structured taxonomy with detailed tables covering tasks, datasets, methods, and model types."
        ],
        "limitations": [
          "Survey nature means no original experimental results or novel model development",
          "Coverage limited to studies available up to publication date (June 2025)",
          "Focus on English-language literature may miss relevant non-English research",
          "No quantitative meta-analysis of performance across different approaches",
          "Limited discussion of real-time implementation challenges for clinical deployment"
        ],
        "method": {
          "architecture": "Transformer-based architectures adapted through fine-tuning, few-shot learning, and zero-shot learning for EEG analysis across four domains.",
          "finetuning": null,
          "objective": null,
          "pretraining": null
        },
        "notes": "{\"chars\": 74261, \"error\": null, \"pages\": 19, \"tool\": \"pypdf\"};input_mode=fulltext;prompt_tokens=20201",
        "one_liner": "Comprehensive survey and taxonomy of LLM applications in EEG analysis across four domains.",
        "open_source": {
          "code_url": null,
          "license": null,
          "weights_url": null
        },
        "paper_type": "survey",
        "published_date": "2025-06-02",
        "tags": {
          "backbone": [
            "transformer"
          ],
          "objective": [
            "masked-reconstruction",
            "autoregressive"
          ],
          "paper_type": [
            "survey"
          ],
          "tokenization": [
            "time-patch",
            "latent-tokens"
          ],
          "topology": [
            "fixed-montage",
            "channel-flexible"
          ]
        },
        "title": "Large Language Models for EEG: A Comprehensive Survey and Taxonomy",
        "unique_contribution": "Provides the first comprehensive taxonomy and systematic review of LLM applications in EEG analysis, organizing recent studies into four distinct domains and highlighting adaptation strategies across the emerging field.",
        "used_fulltext": true
      },
      "summary_failed_reason": null,
      "title": "Large Language Models for EEG: A Comprehensive Survey and Taxonomy",
      "triage": {
        "confidence": 0.95,
        "decision": "accept",
        "reasons": [
          "EEG is central to the survey",
          "Explicitly covers LLM-inspired foundation models for EEG",
          "Focuses on pretrained transferable EEG representations"
        ]
      }
    },
    {
      "arxiv_id": "2506.09110v2",
      "arxiv_id_base": "2506.09110",
      "authors": [
        "Jingying Ma",
        "Feng Wu",
        "Qika Lin",
        "Yucheng Xing",
        "Chenyu Liu",
        "Ziyu Jia",
        "Mengling Feng"
      ],
      "categories": [
        "cs.LG"
      ],
      "links": {
        "abs": "http://arxiv.org/abs/2506.09110v2",
        "pdf": "https://arxiv.org/pdf/2506.09110v2"
      },
      "published_date": "2025-06-10",
      "summary": {
        "arxiv_id_base": "2506.09110",
        "categories": [
          "cs.LG"
        ],
        "data_scale": {
          "channels": 19.0,
          "datasets": [
            "TUEG"
          ],
          "eeg_hours": 9246.0,
          "subjects": 1109545.0
        },
        "detailed_summary": "CodeBrain addresses key limitations in existing EEG foundation models by introducing a two-stage architecture. The first stage employs a TFDual-Tokenizer that decouples heterogeneous temporal and frequency EEG signals into discrete tokens, expanding the representation space and enabling domain-specific interpretability by aligning tokens with neural events and spectral rhythms. The second stage uses a multi-scale EEGSSM architecture that combines structured global convolution with sliding window attention to efficiently capture both sparse long-range and local dependencies, reflecting the brain's small-world topology. Pretrained on the largest public EEG corpus (TUEG), CodeBrain achieves strong generalization across 8 downstream tasks and 10 datasets under distribution shifts, supported by comprehensive ablations, scaling-law analyses, and interpretability evaluations.",
        "evaluation": {
          "benchmarks": [
            "Cohen's Kappa",
            "Weighted F1",
            "Balanced Accuracy",
            "AUROC",
            "AUC-PR"
          ],
          "headline_results": [
            "Strong generalization under distribution shifts",
            "Achieves state-of-the-art performance across 8 downstream tasks"
          ],
          "tasks": [
            "emotion recognition",
            "sleep staging",
            "motor imagery",
            "seizure detection"
          ]
        },
        "key_points": [
          "New EEG foundation model: CodeBrain introduces a two-stage architecture with decoupled temporal-frequency tokenization and brain-inspired multi-scale modeling.",
          "Domain-specific interpretability: TFDual-Tokenizer expands representation space and aligns tokens with neural events and spectral rhythms.",
          "Multi-scale architecture: EEGSSM combines structured global convolution with sliding window attention to capture both long-range and local dependencies efficiently."
        ],
        "limitations": [
          "Limited by the quality and diversity of pretraining data",
          "May require substantial computational resources for pretraining",
          "Interpretability analyses are qualitative and need further validation",
          "Performance on rare classes may be suboptimal due to data imbalance",
          "Channel robustness testing is limited to random masking scenarios"
        ],
        "method": {
          "architecture": "Two-stage architecture: (1) TFDual-Tokenizer with separate temporal/frequency codebooks for discrete tokenization and domain-specific interpretability; (2) EEGSSM with structured global convolution and sliding window attention for efficient multi-scale dependency modeling.",
          "finetuning": "Fine-tuned on 8 downstream tasks across 10 datasets",
          "objective": "Discrete code prediction",
          "pretraining": "Pretrained on TUEG, the largest public EEG corpus"
        },
        "notes": "{\"chars\": 110108, \"error\": null, \"pages\": 43, \"tool\": \"pypdf\"};input_mode=fulltext;prompt_tokens=29220",
        "one_liner": "CodeBrain introduces a two-stage EEG foundation model with decoupled temporal-frequency tokenization and brain-inspired multi-scale architecture for improved interpretability and generalization.",
        "open_source": {
          "code_url": null,
          "license": null,
          "weights_url": null
        },
        "paper_type": "new_model",
        "published_date": "2025-06-10",
        "tags": {
          "backbone": [
            "mamba-ssm"
          ],
          "objective": [
            "discrete-code-prediction"
          ],
          "paper_type": [
            "new-model"
          ],
          "tokenization": [
            "discrete-tokens"
          ],
          "topology": [
            "topology-agnostic"
          ]
        },
        "title": "CodeBrain: Towards Decoupled Interpretability and Multi-Scale Architecture for EEG Foundation Model",
        "unique_contribution": "CodeBrain is the first EEG foundation model to decouple temporal and frequency EEG signals into domain-specific discrete tokens while integrating a brain-inspired multi-scale architecture for efficient global and local dependency modeling.",
        "used_fulltext": true
      },
      "summary_failed_reason": null,
      "title": "CodeBrain: Towards Decoupled Interpretability and Multi-Scale Architecture for EEG Foundation Model",
      "triage": {
        "confidence": 0.95,
        "decision": "accept",
        "reasons": [
          "EEG foundation model (EFM) explicitly stated",
          "Two-stage architecture for EEG pretraining",
          "Pretrained on largest public EEG corpus with downstream generalization"
        ]
      }
    },
    {
      "arxiv_id": "2507.14141v1",
      "arxiv_id_base": "2507.14141",
      "authors": [
        "Danny Dongyeop Han",
        "Ahhyun Lucy Lee",
        "Taeyang Lee",
        "Yonghyeon Gwon",
        "Sebin Lee",
        "Seongjin Lee",
        "David Keetae Park",
        "Shinjae Yoo",
        "Jiook Cha",
        "Chun Kee Chung"
      ],
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "links": {
        "abs": "http://arxiv.org/abs/2507.14141v1",
        "pdf": "https://arxiv.org/pdf/2507.14141v1"
      },
      "published_date": "2025-06-13",
      "summary": {
        "arxiv_id_base": "2507.14141",
        "categories": [
          "eess.SP",
          "cs.AI",
          "cs.LG"
        ],
        "data_scale": {
          "channels": 19.0,
          "datasets": [
            "TUEG"
          ],
          "eeg_hours": 30000.0,
          "subjects": 14987.0
        },
        "detailed_summary": "DIVER-0 addresses critical limitations in existing EEG foundation models by introducing unified spatio-temporal attention that captures complex brain dynamics while maintaining channel permutation equivariance. The model combines Rotary Position Embedding (RoPE) for temporal relationships with binary attention biases for channel differentiation, enabling robust generalization across diverse electrode configurations. A key innovation is the Sliding Temporal Conditional Positional Encoding (STCPE), which preserves both temporal translation equivariance and channel permutation equivariance, allowing the model to adapt to arbitrary electrode configurations unseen during pretraining. Experimental results demonstrate that DIVER-0 achieves competitive performance on emotion recognition and motor imagery tasks using only 10% of the pretraining data while maintaining consistent results across all channel permutation conditions.",
        "evaluation": {
          "benchmarks": [
            "FACED",
            "PhysioNet-MI"
          ],
          "headline_results": [
            "59.2% balanced accuracy on FACED emotion recognition",
            "62.8% balanced accuracy on PhysioNet-MI motor imagery"
          ],
          "tasks": [
            "Emotion recognition",
            "Motor imagery"
          ]
        },
        "key_points": [
          "New EEG foundation model: DIVER-0 achieves competitive performance on emotion recognition and motor imagery tasks using only 10% of pretraining data while maintaining strict channel permutation equivariance.",
          "Unified spatio-temporal attention: Combines Rotary Position Embedding for temporal relationships with binary attention biases for channel differentiation, capturing complex brain dynamics more effectively than segregated spatial-temporal processing.",
          "Sliding Temporal Conditional Positional Encoding: Introduces STCPE that preserves both temporal translation equivariance and channel permutation equivariance, enabling robust generalization to arbitrary electrode configurations unseen during pretraining."
        ],
        "limitations": [
          "Limited evaluation to only two downstream datasets (FACED and PhysioNet-MI)",
          "Performance on motor imagery tasks suggests spatially constrained attention may be more suitable for localized cortical processes",
          "Ablation studies show some configurations achieve slightly higher performance on motor imagery, indicating potential task-specific optimization needs",
          "Relies on binary attention biases which may sacrifice detailed channel-specific discrimination for permutation equivariance",
          "Full potential not explored with complete TUEG corpus (only 10% used in experiments)"
        ],
        "method": {
          "architecture": "Transformer-based with unified spatio-temporal attention",
          "finetuning": "Emotion recognition and motor imagery tasks",
          "objective": "Masked patch reconstruction",
          "pretraining": "10% of TUEG corpus"
        },
        "notes": "{\"chars\": 41638, \"error\": null, \"pages\": 11, \"tool\": \"pypdf\"};input_mode=fulltext;prompt_tokens=11861",
        "one_liner": "DIVER-0 is a novel EEG foundation model that achieves competitive performance with only 10% of pretraining data while maintaining strict channel permutation equivariance for robust cross-dataset generalization.",
        "open_source": {
          "code_url": "https://github.com/cha-lab/DIVER-0",
          "license": "Unknown",
          "weights_url": "https://github.com/cha-lab/DIVER-0"
        },
        "paper_type": "new_model",
        "published_date": "2025-06-13",
        "tags": {
          "backbone": [
            "transformer"
          ],
          "objective": [
            "masked-reconstruction"
          ],
          "paper_type": [
            "new-model"
          ],
          "tokenization": [
            "time-patch"
          ],
          "topology": [
            "channel-flexible"
          ]
        },
        "title": "DIVER-0 : A Fully Channel Equivariant EEG Foundation Model",
        "unique_contribution": "DIVER-0 introduces unified spatio-temporal attention with binary attention biases and STCPE to achieve strict channel permutation equivariance while maintaining competitive performance with minimal pretraining data.",
        "used_fulltext": true
      },
      "summary_failed_reason": null,
      "title": "DIVER-0 : A Fully Channel Equivariant EEG Foundation Model",
      "triage": {
        "confidence": 1.0,
        "decision": "accept",
        "reasons": [
          "EEG is central modality",
          "Explicitly proposes EEG foundation model",
          "Focuses on pretraining for broad transfer",
          "Addresses generalization across electrode configurations"
        ]
      }
    },
    {
      "arxiv_id": "2506.16056v1",
      "arxiv_id_base": "2506.16056",
      "authors": [
        "Puchun Liu",
        "C. L. Philip Chen",
        "Yubin He",
        "Tong Zhang"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "links": {
        "abs": "http://arxiv.org/abs/2506.16056v1",
        "pdf": "https://arxiv.org/pdf/2506.16056v1"
      },
      "published_date": "2025-06-19",
      "summary": {
        "arxiv_id_base": "2506.16056",
        "categories": [
          "cs.LG",
          "cs.AI"
        ],
        "data_scale": {
          "channels": null,
          "datasets": [
            "Temple University EEG corpus",
            "CHB-MIT dataset"
          ],
          "eeg_hours": null,
          "subjects": null
        },
        "detailed_summary": "This paper proposes CRIA, a novel pre-training framework for EEG representation learning that addresses the challenge of extracting deep features from multi-view EEG data. CRIA introduces an asymmetric three-view interaction mechanism where spectral view is the dominant modality, employing cross-attention to fuse temporal, spectral, and spatial features. The framework uses variable-length and variable-channel coding to handle data heterogeneity across different EEG datasets, and incorporates a view-wise masking pre-training strategy based on information bottleneck principles. Experimental results on the Temple University EEG corpus and CHB-MIT dataset demonstrate that CRIA outperforms existing methods, achieving 57.02% balanced accuracy for multi-class event classification and 80.03% for anomaly detection.",
        "evaluation": {
          "benchmarks": [],
          "headline_results": [],
          "tasks": []
        },
        "key_points": [
          "New EEG foundation model: CRIA introduces a cross-view interaction and instance-adapted pre-training framework for generalizable EEG representations.",
          "Method novelty: Employs asymmetric three-view interaction with spectral view as dominant modality and cross-attention mechanism for feature fusion.",
          "Strongest evidence: Achieves 57.02% balanced accuracy for multi-class event classification and 80.03% for anomaly detection on benchmark datasets."
        ],
        "limitations": [
          "Practical effects and potential challenges in clinical applications need further exploration",
          "Impact of diverse pathologic conditions on generalizability not fully characterized",
          "Trade-off between computational efficiency and interpretability in real-time integration not addressed",
          "Patient specificity and individual variability not explicitly modeled",
          "Large-scale pre-training and real-time processing studies still needed"
        ],
        "method": {
          "architecture": null,
          "finetuning": null,
          "objective": null,
          "pretraining": null
        },
        "notes": "{\"chars\": 68526, \"error\": null, \"pages\": null, \"tool\": \"cached\"};input_mode=fulltext_slices;reason=fulltext_over_limit;prompt_tokens=18745;max_tokens=1",
        "one_liner": "CRIA is a cross-view interaction and instance-adapted pre-training framework for generalizable EEG representations.",
        "open_source": {
          "code_url": null,
          "license": null,
          "weights_url": null
        },
        "paper_type": "new_model",
        "published_date": "2025-06-19",
        "tags": {
          "backbone": [
            "transformer"
          ],
          "objective": [
            "masked-reconstruction"
          ],
          "paper_type": [
            "new-model"
          ],
          "tokenization": [
            "time-patch"
          ],
          "topology": [
            "channel-flexible"
          ]
        },
        "title": "CRIA: A Cross-View Interaction and Instance-Adapted Pre-training Framework for Generalizable EEG Representations",
        "unique_contribution": "CRIA is the first EEG pre-training strategy to model view fusion from a dominantâ€“auxiliary modality perspective, introducing asymmetric three-view lateral interaction with spectral view as dominant modality.",
        "used_fulltext": true
      },
      "summary_failed_reason": null,
      "title": "CRIA: A Cross-View Interaction and Instance-Adapted Pre-training Framework for Generalizable EEG Representations",
      "triage": {
        "confidence": 0.95,
        "decision": "accept",
        "reasons": [
          "proposes CRIA, a generalizable pretraining framework for EEG representation learning",
          "explicitly frames work as pretraining for broad transfer and generalization",
          "outperforms existing methods in multi-class event classification and anomaly detection"
        ]
      }
    },
    {
      "arxiv_id": "2506.17068v1",
      "arxiv_id_base": "2506.17068",
      "authors": [
        "Runkai Zhang",
        "Hua Yu",
        "John Q. Gan",
        "Haixian Wang"
      ],
      "categories": [
        "q-bio.NC",
        "cs.ET",
        "eess.SP"
      ],
      "links": {
        "abs": "http://arxiv.org/abs/2506.17068v1",
        "pdf": "https://arxiv.org/pdf/2506.17068v1"
      },
      "published_date": "2025-06-20",
      "summary": {
        "arxiv_id_base": "2506.17068",
        "categories": [
          "q-bio.NC",
          "cs.ET",
          "eess.SP"
        ],
        "data_scale": {
          "channels": null,
          "datasets": [
            "TASMC-UCLA",
            "CHB-MIT",
            "MAYO",
            "others"
          ],
          "eeg_hours": 2741.1,
          "subjects": 1199
        },
        "detailed_summary": "This paper introduces EpiNT, a novel Transformer-based pre-trained model designed to address the challenges of unified EEG and iEEG analysis. EpiNT employs channel-independent modeling with masked autoencoders (MAE) and vector quantization (VQ), along with a frequency domain mapping quantizer to capture crucial frequency features. Pre-trained on over 2,700 hours of multi-modal clinical neurophysiological data from 1,199 patients, EpiNT outperformed both randomly initialized models and other pre-trained methods on six downstream classification tasks, demonstrating robust representation learning capabilities.",
        "evaluation": {
          "benchmarks": [
            "TASMC-UCLA",
            "CHB-MIT",
            "MAYO",
            "others"
          ],
          "headline_results": [
            "F1_score: 0.925 (TASMC-UCLA), 0.913 (CHB-MIT), 0.965 (MAYO)",
            "Outperformed MAE-EEG, Brant, MOMENT, PatchTST, and VQ-MTM"
          ],
          "tasks": [
            "seizure detection",
            "seizure type classification",
            "sleep staging",
            "others"
          ]
        },
        "key_points": [
          "New EEG foundation model: EpiNT uses masked autoencoders and vector quantization for unified EEG/iEEG analysis",
          "Novel frequency domain quantizer captures modality-specific spectral features while addressing amplitude and SNR differences",
          "Strong performance across six clinical tasks with cross-subject generalization on over 2,700 hours of multi-modal data"
        ],
        "limitations": [
          "Performance variability across tasks suggests need for task-specific fine-tuning strategies",
          "Domain shift between pre-training and downstream datasets may affect generalization",
          "Class imbalance in clinical data presents challenges for robust model training"
        ],
        "method": {
          "architecture": "Transformer encoder with masked autoencoders and vector quantization",
          "finetuning": "Cross-entropy loss with linear-probing, last-layer, or full-parameter tuning",
          "objective": "Masked signal reconstruction with frequency domain quantization",
          "pretraining": "Unsupervised pre-training on 2,741.1 hours of EEG/iEEG data from 1,199 patients"
        },
        "notes": "{\"chars\": 77176, \"error\": null, \"pages\": null, \"tool\": \"cached\"};input_mode=fulltext;prompt_tokens=20647",
        "one_liner": "EpiNT is a Transformer-based pre-trained model for unified EEG and iEEG analysis using masked autoencoders and vector quantization.",
        "open_source": {
          "code_url": "https://github.com/RunKZhang/EpiNT",
          "license": "Unknown",
          "weights_url": "Available"
        },
        "paper_type": "new_model",
        "published_date": "2025-06-20",
        "tags": {
          "backbone": [
            "transformer"
          ],
          "objective": [
            "masked-reconstruction",
            "contrastive"
          ],
          "paper_type": [
            "new-model"
          ],
          "tokenization": [
            "time-patch"
          ],
          "topology": [
            "channel-flexible"
          ]
        },
        "title": "Cross-Modal Epileptic Signal Harmonization: Frequency Domain Mapping Quantization for Pre-training a Unified Neurophysiological Transformer",
        "unique_contribution": "EpiNT is the first pre-trained Transformer model specifically designed for unified EEG and iEEG analysis, addressing modality-specific challenges through channel-independent modeling and frequency domain quantization.",
        "used_fulltext": true
      },
      "summary_failed_reason": null,
      "title": "Cross-Modal Epileptic Signal Harmonization: Frequency Domain Mapping Quantization for Pre-training a Unified Neurophysiological Transformer",
      "triage": {
        "confidence": 0.95,
        "decision": "accept",
        "reasons": [
          "EEG and iEEG are central modalities",
          "Introduces a Transformer-based pre-trained model (EpiNT)",
          "Uses masked autoencoders and vector quantization for foundation model-style pretraining",
          "Demonstrates robust representation learning on downstream tasks"
        ]
      }
    },
    {
      "arxiv_id": "2506.20354v2",
      "arxiv_id_base": "2506.20354",
      "authors": [
        "Francesco Carzaniga",
        "Michael Hersche",
        "Abu Sebastian",
        "Kaspar Schindler",
        "Abbas Rahimi"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "links": {
        "abs": "http://arxiv.org/abs/2506.20354v2",
        "pdf": "https://arxiv.org/pdf/2506.20354v2"
      },
      "published_date": "2025-06-25",
      "summary": {
        "arxiv_id_base": "2506.20354",
        "categories": [
          "cs.LG",
          "cs.AI"
        ],
        "data_scale": {
          "channels": null,
          "datasets": [
            "SWEC iEEG",
            "MAYO",
            "FNUSA",
            "Brain TreeBank"
          ],
          "eeg_hours": 10000.0,
          "subjects": 68.0
        },
        "detailed_summary": "This paper introduces MVPFormer, a generative foundation model for human electrophysiology, powered by a novel multi-variate parallel attention (MVPA) mechanism. MVPFormer is trained on the SWEC iEEG dataset, the largest publicly available iEEG corpus to date with nearly 10,000 hours of recordings. The model demonstrates expert-level performance in seizure detection across multiple institutional datasets and achieves state-of-the-art results on four Brain TreeBank iEEG decoding tasks. MVPA enables flexible and efficient modeling of time-series data with varying channel counts and configurations by disentangling content, temporal, and spatial attention. The work establishes MVPFormer as the first open-source, open-weights, and open-data iEEG foundation model with SOTA clinical performance.",
        "evaluation": {
          "benchmarks": [
            "SWEC iEEG",
            "Brain TreeBank"
          ],
          "headline_results": [
            "Kappa score of 0.57 on seizure detection",
            "SOTA performance on four Brain TreeBank tasks"
          ],
          "tasks": [
            "Seizure detection",
            "Brain TreeBank iEEG decoding"
          ]
        },
        "key_points": [
          "New EEG foundation model: MVPFormer uses multi-variate parallel attention (MVPA) to achieve expert-level seizure detection and SOTA performance on iEEG tasks.",
          "Novel attention mechanism: MVPA disentangles content, temporal, and spatial attention, enabling flexible modeling of time-series with varying channel counts and configurations.",
          "Largest iEEG dataset: Releases the SWEC iEEG dataset, comprising nearly 10,000 hours of recordings from heterogeneous clinical sources, supporting community foundation model efforts."
        ],
        "limitations": [
          "The SWEC iEEG dataset, while large, may not be sufficient to fully train very large models according to scaling laws.",
          "The model's performance may be affected by the quality of the data, as high-quality data is crucial for optimal performance.",
          "The model is developed for research purposes and may not be used for diagnostic purposes."
        ],
        "method": {
          "architecture": "MVPFormer with MVPA",
          "finetuning": "LoRA for downstream classification tasks",
          "objective": "Generative pre-training with contrastive loss",
          "pretraining": "Trained on SWEC iEEG dataset"
        },
        "notes": "{\"chars\": 144420, \"error\": null, \"pages\": 59, \"tool\": \"pypdf\"};input_mode=fulltext;prompt_tokens=38149",
        "one_liner": "MVPFormer is the first open-source, open-weights, and open-data iEEG foundation model with SOTA clinical performance.",
        "open_source": {
          "code_url": "https://github.com/IBM/multi-variate-parallel-transformer",
          "license": "open-source",
          "weights_url": "open-weights"
        },
        "paper_type": "new_model",
        "published_date": "2025-06-25",
        "tags": {
          "backbone": [
            "transformer"
          ],
          "objective": [
            "autoregressive"
          ],
          "paper_type": [
            "new-model"
          ],
          "tokenization": [
            "time-patch"
          ],
          "topology": [
            "channel-flexible"
          ]
        },
        "title": "A foundation model with multi-variate parallel attention to generate neuronal activity",
        "unique_contribution": "Introduces MVPA, a novel self-attention mechanism that disentangles content, temporal, and spatial attention, enabling flexible and efficient modeling of heterogeneous time-series data, and applies it to build MVPFormer, the first open-source, open-weights, and open-data iEEG foundation model with SOTA clinical performance.",
        "used_fulltext": true
      },
      "summary_failed_reason": null,
      "title": "A foundation model with multi-variate parallel attention to generate neuronal activity",
      "triage": {
        "confidence": 1.0,
        "decision": "accept",
        "reasons": [
          "EEG is central (iEEG) and explicitly framed as foundation model",
          "Introduces MVPFormer, a generative foundation model for electrophysiology",
          "Trained for broad transfer across diverse subjects and tasks",
          "Releases largest iEEG dataset to support community FM efforts"
        ]
      }
    },
    {
      "arxiv_id": "2506.23075v1",
      "arxiv_id_base": "2506.23075",
      "authors": [
        "Yuchen Zhou",
        "Jiamin Wu",
        "Zichen Ren",
        "Zhouheng Yao",
        "Weiheng Lu",
        "Kunyu Peng",
        "Qihao Zheng",
        "Chunfeng Song",
        "Wanli Ouyang",
        "Chao Gou"
      ],
      "categories": [
        "cs.HC",
        "cs.LG",
        "eess.SP",
        "q-bio.NC"
      ],
      "links": {
        "abs": "http://arxiv.org/abs/2506.23075v1",
        "pdf": "https://arxiv.org/pdf/2506.23075v1"
      },
      "published_date": "2025-06-29",
      "summary": {
        "arxiv_id_base": "2506.23075",
        "categories": [
          "cs.HC",
          "cs.LG",
          "eess.SP",
          "q-bio.NC"
        ],
        "data_scale": {
          "channels": 0.0,
          "datasets": [
            "datasets"
          ],
          "eeg_hours": 9000.0,
          "subjects": 0.0
        },
        "detailed_summary": "CSBrain addresses a fundamental limitation in existing EEG foundation models: their reliance on scale-agnostic dense modeling inherited from NLP and vision, which fails to capture the intrinsic cross-scale spatiotemporal structure of neural activity. The model introduces two key innovations: Cross-scale Spatiotemporal Tokenization (CST), which aggregates multi-scale features from localized temporal windows and anatomical brain regions into compact scale-aware tokens, and Structured Sparse Attention (SSA), which captures cross-window and cross-region dependencies while eliminating spurious correlations. These components are alternately stacked to progressively integrate multi-scale dependencies. Extensive experiments across 11 representative EEG tasks and 16 public datasets demonstrate that CSBrain consistently outperforms both task-specific models and strong foundation model baselines, establishing cross-scale modeling as a key inductive bias for generalized EEG decoding.",
        "evaluation": {
          "benchmarks": [
            "datasets"
          ],
          "headline_results": [
            "balanced accuracy",
            "Cohen's kappa",
            "weighted F1",
            "AUC-PR",
            "AUROC",
            "Pearson correlation",
            "R2 score",
            "RMSE"
          ],
          "tasks": [
            "motor imagery classification",
            "emotion recognition",
            "seizure detection",
            "sleep staging",
            "imagined speech classification",
            "vigilance estimation",
            "mental stress detection",
            "mental disorder diagnosis",
            "event type classification",
            "abnormal detection",
            "slowing event classification"
          ]
        },
        "key_points": [
          "New EEG foundation model: CSBrain introduces cross-scale spatiotemporal modeling for generalized brain decoding across diverse tasks.",
          "Novel architecture: Combines Cross-scale Spatiotemporal Tokenization (CST) with Structured Sparse Attention (SSA) to capture multi-scale neural patterns while avoiding spurious dependencies.",
          "Strong empirical results: Achieves state-of-the-art performance across 11 tasks and 16 datasets, outperforming both task-specific and foundation model baselines."
        ],
        "limitations": [
          "Computational resource constraints limit exploration of scaling behavior at the scale of vision-language or language-only models",
          "Open question remains about how much EEG data is sufficient to pretrain high-quality foundation models",
          "Sparse, noisy, and heterogeneous nature of EEG datasets requires further work on large-scale dataset unification and normalization"
        ],
        "method": {
          "architecture": "Cross-scale Spatiotemporal Tokenization (CST) aggregates multi-scale features from localized temporal windows and anatomical brain regions into compact scale-aware tokens. Structured Sparse Attention (SSA) captures cross-window and cross-region dependencies while eliminating spurious correlations. CST and SSA are alternately stacked for L layers to progressively integrate cross-scale spatiotemporal dependencies.",
          "finetuning": null,
          "objective": null,
          "pretraining": null
        },
        "notes": "{\"chars\": 127123, \"error\": null, \"pages\": 37, \"tool\": \"pypdf\"};input_mode=fulltext;prompt_tokens=33573",
        "one_liner": "CSBrain introduces cross-scale spatiotemporal modeling to EEG foundation models, achieving state-of-the-art performance across 11 tasks and 16 datasets.",
        "open_source": {
          "code_url": null,
          "license": null,
          "weights_url": null
        },
        "paper_type": "new_model",
        "published_date": "2025-06-29",
        "tags": {
          "backbone": [
            "transformer"
          ],
          "objective": [
            "masked-reconstruction"
          ],
          "paper_type": [
            "new-model"
          ],
          "tokenization": [
            "time-patch"
          ],
          "topology": [
            "fixed-montage"
          ]
        },
        "title": "CSBrain: A Cross-scale Spatiotemporal Brain Foundation Model for EEG Decoding",
        "unique_contribution": "CSBrain is the first EEG foundation model to explicitly model cross-scale spatiotemporal structure through alternating CST and SSA modules, achieving state-of-the-art performance across diverse brain decoding tasks.",
        "used_fulltext": true
      },
      "summary_failed_reason": null,
      "title": "CSBrain: A Cross-scale Spatiotemporal Brain Foundation Model for EEG Decoding",
      "triage": {
        "confidence": 0.95,
        "decision": "accept",
        "reasons": [
          "explicitly proposes EEG foundation model (CSBrain)",
          "focuses on generalized EEG decoding via large-scale pretraining",
          "introduces novel cross-scale modeling for EEG-FM"
        ]
      }
    }
  ],
  "stats": {
    "accepted": 8,
    "candidates": 20,
    "summarized": 8
  },
  "top_picks": [
    "2507.14141",
    "2506.20354",
    "2506.01867",
    "2506.23075",
    "2506.17068"
  ]
}

{
  "month": "2024-10",
  "papers": [
    {
      "arxiv_id": "2410.19779v2",
      "arxiv_id_base": "2410.19779",
      "authors": [
        "Tongtian Yue",
        "Xuange Gao",
        "Shuning Xue",
        "Yepeng Tang",
        "Longteng Guo",
        "Jie Jiang",
        "Jing Liu"
      ],
      "categories": [
        "eess.SP",
        "cs.LG"
      ],
      "links": {
        "abs": "http://arxiv.org/abs/2410.19779v2",
        "pdf": "https://arxiv.org/pdf/2410.19779v2"
      },
      "published_date": "2024-10-14",
      "summary": {
        "arxiv_id_base": "2410.19779",
        "categories": [
          "eess.SP",
          "cs.LG"
        ],
        "data_scale": {
          "channels": 138.0,
          "datasets": [
            "12"
          ],
          "eeg_hours": null,
          "subjects": null
        },
        "detailed_summary": "BrainGPT addresses the limitations of specialist EEG models by introducing an electrode-wise modeling strategy that treats each electrode as an independent sample, enabling integration of diverse EEG datasets with up to 138 electrodes. It employs autoregressive pre-training instead of traditional masked autoencoder approaches, capturing temporal dependencies through next-token prediction. The model scales up to 1.1B parameters and introduces a multi-task transfer learning paradigm using a learnable electrode graph network shared across tasks. BrainGPT demonstrates broad compatibility with various signal acquisition devices, subjects, and tasks, outperforming existing specialist models across 12 benchmarks spanning 5 distinct tasks.",
        "evaluation": {
          "benchmarks": [
            "12 benchmarks"
          ],
          "headline_results": [
            "State-of-the-art performance across all tasks"
          ],
          "tasks": [
            "Emotion Recognition",
            "Motor Imagery",
            "Mental Workload",
            "Sleeping Stage",
            "Cross Modality"
          ]
        },
        "key_points": [
          "New EEG foundation model: BrainGPT introduces the first generalist EEG foundation model using autoregressive pre-training and electrode-wise modeling to unify diverse EEG datasets.",
          "Electrode-wise modeling strategy: Treats each electrode as an independent sample, enabling integration of up to 138 electrodes and arbitrary combinations for flexible data processing.",
          "Multi-task transfer learning: Introduces a learnable electrode graph network shared across tasks, demonstrating confirmed multi-task compatibility and synergistic performance improvements."
        ],
        "limitations": [
          "Limited discussion of computational requirements for large-scale deployment",
          "No explicit comparison with other foundation model approaches on cross-dataset generalization",
          "Potential domain mismatch issues when transferring from seizure data to general EEG tasks not fully addressed"
        ],
        "method": {
          "architecture": "Electrode Temporal Encoder (ETE) with transformer backbone",
          "finetuning": "Multi-task transfer learning with Task-shared Electrode Graph (TEG) network",
          "objective": "Autoregressive next-token prediction",
          "pretraining": "Electrode-wise pre-training on diverse EEG datasets"
        },
        "notes": "{\"chars\": 53041, \"error\": null, \"pages\": 10, \"tool\": \"pypdf\"};input_mode=fulltext;prompt_tokens=14937",
        "one_liner": "BrainGPT is the first generalist EEG foundation model using autoregressive pre-training to achieve state-of-the-art performance across 12 benchmarks spanning 5 tasks.",
        "open_source": {
          "code_url": null,
          "license": null,
          "weights_url": null
        },
        "paper_type": "new_model",
        "published_date": "2024-10-14",
        "tags": {
          "backbone": [
            "transformer"
          ],
          "objective": [
            "autoregressive"
          ],
          "paper_type": [
            "new-model"
          ],
          "tokenization": [
            "time-patch"
          ],
          "topology": [
            "channel-flexible"
          ]
        },
        "title": "BrainGPT: Unleashing the Potential of EEG Generalist Foundation Model by Autoregressive Pre-training",
        "unique_contribution": "BrainGPT is the first generalist EEG foundation model that unifies diverse EEG datasets through electrode-wise modeling and autoregressive pre-training, achieving state-of-the-art performance across multiple tasks while demonstrating confirmed multi-task compatibility and synergy.",
        "used_fulltext": true
      },
      "summary_failed_reason": null,
      "title": "BrainGPT: Unleashing the Potential of EEG Generalist Foundation Model by Autoregressive Pre-training",
      "triage": {
        "confidence": 1.0,
        "decision": "accept",
        "reasons": [
          "Introduces EEGPT, the first generalist EEG foundation model",
          "Uses autoregressive pre-training for broad transfer",
          "Evaluates on 5 tasks across 12 benchmarks outperforming specialist models"
        ]
      }
    },
    {
      "arxiv_id": "2410.19842v1",
      "arxiv_id_base": "2410.19842",
      "authors": [
        "Thea Brüsch",
        "Mikkel N. Schmidt",
        "Tommy S. Alstrøm"
      ],
      "categories": [
        "eess.SP",
        "cs.LG"
      ],
      "links": {
        "abs": "http://arxiv.org/abs/2410.19842v1",
        "pdf": "https://arxiv.org/pdf/2410.19842v1"
      },
      "published_date": "2024-10-21",
      "summary": {
        "arxiv_id_base": "2410.19842",
        "categories": [
          "eess.SP",
          "cs.LG"
        ],
        "data_scale": {
          "channels": null,
          "datasets": [
            "Physionet Challenge 2018 (EEG)",
            "SleepEDFx (EEG)",
            "MASS (EEG)",
            "Physionet Challenge 2021 (ECG)"
          ],
          "eeg_hours": null,
          "subjects": null
        },
        "detailed_summary": "This paper addresses the challenge of self-supervised learning for multivariate biosignals where the number of input channels varies across applications. The authors introduce contrastive random lead coding (CRLC), a method that creates positive pairs by randomly sampling subsets of input channels, enabling channel-agnostic pretraining. They validate CRLC on EEG and ECG data, showing it outperforms competing strategies including contrastive segment coding and contrastive augmented coding. Notably, CRLC surpasses the current state-of-the-art reference model for EEG tasks while achieving comparable results for ECG. The approach leverages a message passing neural network (MPNN) architecture that can handle variable channel configurations without discarding inter-channel information.",
        "evaluation": {
          "benchmarks": [
            "SleepEDFx",
            "MASS",
            "Chapman Shaoxing"
          ],
          "headline_results": [
            "CRLC outperforms state-of-the-art reference model for EEG tasks",
            "CRLC achieves comparable results for ECG tasks",
            "Evaluated with balanced accuracy across multiple sample sizes"
          ],
          "tasks": [
            "EEG sleep staging",
            "ECG classification"
          ]
        },
        "key_points": [
          "New EEG foundation model: Introduces contrastive random lead coding (CRLC) for channel-agnostic self-supervision of biosignals",
          "Method novelty: Uses random subsets of input channels to create positive pairs, enabling transfer across variable channel setups",
          "Strongest evidence: CRLC outperforms state-of-the-art reference model for EEG tasks and achieves comparable results for ECG"
        ],
        "limitations": [
          "State-of-the-art reference model (W2V+CMSC+RLM) still outperforms proposed method for ECG tasks",
          "MPNN setup uses fully connected graph which may overfit on EEG data",
          "Models pretrained on fixed channel datasets (6 EEG channels, 12 ECG leads) may not generalize to all possible channel configurations",
          "No investigation of optimal graph structures for biosignals beyond fully connected"
        ],
        "method": {
          "architecture": "Channel-agnostic model architecture using convolutional encoder + MPNN",
          "finetuning": null,
          "objective": "CRLC creates positive pairs by randomly sampling channel subsets",
          "pretraining": "compared with CSC (temporal segments) and CAC (augmentations); trained with NT-Xent and TS2Vec contrastive losses"
        },
        "notes": "manual_resummary_prompt_update;input_mode=fulltext;prompt_tokens=19329",
        "one_liner": "Contrastive random lead coding (CRLC) enables channel-agnostic self-supervision of biosignals by using random subsets of channels as positive pairs.",
        "open_source": {
          "code_url": "https://github.com/theabrusch/Multiview_TS_SSL",
          "license": null,
          "weights_url": null
        },
        "paper_type": "new_model",
        "published_date": "2024-10-21",
        "tags": {
          "backbone": [
            "transformer"
          ],
          "objective": [
            "contrastive"
          ],
          "paper_type": [
            "new-model"
          ],
          "tokenization": [
            "time-patch"
          ],
          "topology": [
            "channel-flexible"
          ]
        },
        "title": "Contrastive random lead coding for channel-agnostic self-supervision of biosignals",
        "unique_contribution": "The paper introduces contrastive random lead coding (CRLC) as a novel strategy for creating positive pairs in contrastive learning of multivariate biosignals, enabling channel-agnostic pretraining that outperforms existing methods and state-of-the-art reference models.",
        "used_fulltext": true
      },
      "summary_failed_reason": null,
      "title": "Contrastive random lead coding for channel-agnostic self-supervision of biosignals",
      "triage": {
        "confidence": 0.9,
        "decision": "accept",
        "reasons": [
          "proposes CRLC for channel-agnostic self-supervision of biosignals",
          "validates on EEG with pretraining and fine-tuning",
          "outperforms state-of-the-art reference model for EEG tasks",
          "focuses on generalization across variable channel setups"
        ]
      }
    }
  ],
  "stats": {
    "accepted": 2,
    "candidates": 17,
    "summarized": 2
  },
  "top_picks": [
    "2410.19779",
    "2410.19842"
  ]
}
